This study addresses the complexities in topic modeling, a widely utilized method for analyzing extensive document collections. Despite its popularity, topic modeling faces significant challenges, particularly in algorithm comparison and result evaluation. Existing literature often relies on varied datasets and criteria, leading to inconclusive comparisons of different algorithms. Furthermore, the metrics currently employed to assess topic modeling outputs offer inconsistent insights, complicating the validation of their accuracy. Recognizing these challenges, our research aims to provide clarity in two critical areas. Firstly, we conduct a comprehensive comparison of all prevalent, non-application-specific topic modeling algorithms, including LSA, pLSA, LDA, NMF, and BERTTopic. This comparison is unique as it benchmarks against a known clustering, facilitating an unbiased evaluation of the algorithms' performance. Secondly, our study seeks to establish a definitive ranking of these algorithms based on their accuracy. While our findings contribute to a better understanding of the efficacy and limitations of current topic modeling techniques, we also acknowledge the complexities involved and suggest that the choice of the most suitable algorithm may still depend on specific contextual needs. Our research offers a step towards enhancing the theoretical and practical applications of topic modeling, aiming for more nuanced and effective analysis of large-scale textual data.
