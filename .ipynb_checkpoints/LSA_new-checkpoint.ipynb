{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA) Overview\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a foundational technique in natural language processing and information retrieval. It identifies patterns in the relationships between terms and documents and uncovers latent semantic structures, effectively grouping together terms that are used in similar contexts.\n",
    "\n",
    "### Steps in LSA:\n",
    "\n",
    "1. **Term-Document Matrix Creation**:\n",
    "   LSA constructs a matrix that represents the frequency of terms (words) across a set of documents. The matrix entries may be raw counts or, more commonly, weighted frequencies such as TF-IDF scores.\n",
    "\n",
    "2. **Matrix Decomposition**:\n",
    "   The term-document matrix is decomposed using Singular Value Decomposition (SVD). SVD separates the matrix into three components: a term-concept matrix, a diagonal matrix of singular values, and a concept-document matrix.\n",
    "\n",
    "3. **Dimensionality Reduction**:\n",
    "   By selecting the top `k` singular values and their corresponding vectors, LSA reduces the dimensionality of the term and document space to the `k` most informative concepts. This step helps in denoising the data and clarifying the structure.\n",
    "\n",
    "4. **Concept Identification**:\n",
    "   In the reduced `k`-dimensional space, terms and documents are now associated with latent concepts, which can often be interpreted as topics. The proximity of terms and documents within this space indicates their semantic similarity.\n",
    "\n",
    "5. **Similarity Measurement**:\n",
    "   LSA allows for the measurement of semantic similarity between terms and documents by using the cosine similarity of their vectors in the reduced space. Small angles between vectors indicate a high degree of semantic similarity.\n",
    "\n",
    "LSA is particularly adept at dealing with synonymy and polysemyâ€”common challenges in language processing. However, it does not account for word order or syntactic nuances, and the choice of `k` is crucial for the method's effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "importing libraries and preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a1LAka_8B4v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Loading the data\n",
    "data = pd.read_csv(\"/content/train_no_simplify.csv\")\n",
    "\n",
    "# Extracting the cleaned text\n",
    "texts = data['clean_text'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization and LSA Application:\n",
    "\n",
    "- Vectorizes the cleaned text using TF-IDF with specified max and min document frequency thresholds, excluding common English stop words.\n",
    "- Apply LSA using TruncatedSVD to the TF-IDF matrix to identify latent topics within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czaonStb9Mhp"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Define the number of topics\n",
    "n_topics = 20\n",
    "\n",
    "# Vectorize the cleaned text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=100, min_df=5, stop_words='english')\n",
    "dtm_tfidf = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Applying LSA (Truncated SVD) on the TF-IDF matrix\n",
    "lsa_model = TruncatedSVD(n_components=n_topics, n_iter=10)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(dtm_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction and Coherence Calculation:\n",
    "\n",
    "- Prepares the data for coherence score calculation by tokenizing the texts and creating a Gensim dictionary.\n",
    "- Identifies the top words from each LSA topic.\n",
    "- Constructs a list of topics with top words and calculates the coherence score using Gensim's CoherenceModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yBnzgC39vM6"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Ensure texts_list is a list of lists of tokens\n",
    "texts_list = [text.split() for text in texts]\n",
    "gensim_dictionary = Dictionary(texts_list)\n",
    "\n",
    "\n",
    "n_top_words = 10\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "top_words = [words[np.argsort(topic)[-n_top_words:]] for topic in lsa_model.components_]\n",
    "\n",
    "# Create the topics list expected by CoherenceModel\n",
    "topics = [list(topic) for topic in top_words]\n",
    "cm = CoherenceModel(topics=topics, texts=texts_list, dictionary=gensim_dictionary, coherence='c_v')\n",
    "coherence_score = cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DydQ6x8p9ouy",
    "outputId": "6338506d-38c4-4bb9-bd8c-a90db0362696"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4757086283363246"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsIydz4-_r8P",
    "outputId": "38c19efe-3f1c-4464-961a-96eba9c9dafe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6288398456379947\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_topic_exclusivity(model, feature_names, top_n_words=20):\n",
    "    \"\"\"Calculates the topic exclusivity score for a given topic model.\n",
    "\n",
    "    Args:\n",
    "        model: The fitted topic model with a `components_` attribute containing topic-word distributions.\n",
    "        feature_names: A list of feature names corresponding to the columns of the topic-word matrix.\n",
    "        top_n_words: The number of top words to consider for exclusivity calculation (default: 20).\n",
    "\n",
    "    Returns:\n",
    "        The overall topic exclusivity score, averaged across all topics.\n",
    "    \"\"\"\n",
    "\n",
    "    topics = model.components_\n",
    "    exclusivity_scores = []\n",
    "\n",
    "    for topic_idx, topic in enumerate(topics):\n",
    "        top_features_ind = topic.argsort()[:-top_n_words-1:-1]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "\n",
    "        other_topics = np.delete(topics, topic_idx, axis=0)\n",
    "\n",
    "        # Check for zero denominator and handle it appropriately\n",
    "        if np.sum(other_topics[:, top_features_ind]) == 0:\n",
    "            topic_exclusivity_score = np.inf  # Assign infinite exclusivity if no overlap\n",
    "        else:\n",
    "            topic_exclusivity_score = np.sum(topic[top_features_ind]) / np.sum(other_topics[:, top_features_ind])\n",
    "\n",
    "        exclusivity_scores.append(topic_exclusivity_score)\n",
    "\n",
    "    # Use a robust averaging method to handle potential outliers\n",
    "    overall_exclusivity = np.median(exclusivity_scores)  # Consider np.mean as well\n",
    "\n",
    "    return overall_exclusivity\n",
    "\n",
    "# Calculating Topic Exclusivity\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_exclusivity_score = calculate_topic_exclusivity(lsa_model, feature_names)\n",
    "print(topic_exclusivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnYEmceBEma-",
    "outputId": "cab89366-d648-4d7e-e55c-c9046174c52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Topic Divergence: inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "def calculate_average_topic_divergence(model):\n",
    "    topics = model.components_  # Access topic-word distributions\n",
    "    divergence_matrix = np.zeros((len(topics), len(topics)))\n",
    "\n",
    "    for i in range(len(topics)):\n",
    "        for j in range(i + 1, len(topics)):\n",
    "            divergence = rel_entr(topics[i], topics[j])\n",
    "\n",
    "            # Handle zero denominator and potential numerical issues:\n",
    "            if np.isinf(divergence).any():  # Check for infinity\n",
    "                divergence_matrix[i, j] = np.inf  # Assign infinity\n",
    "            else:\n",
    "                divergence_matrix[i, j] = divergence\n",
    "\n",
    "            divergence_matrix[j, i] = divergence_matrix[i, j]\n",
    "\n",
    "    # Calculate average divergence, ignoring infinity values:\n",
    "    average_divergence = np.nanmean(divergence_matrix[~np.eye(divergence_matrix.shape[0], dtype=bool)])\n",
    "\n",
    "    return average_divergence\n",
    "\n",
    "\n",
    "\n",
    "average_divergence = calculate_average_topic_divergence(lsa_model)  # Adjust the model argument as needed\n",
    "print(\"Average Topic Divergence:\", average_divergence)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
